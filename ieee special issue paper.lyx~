#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass IEEEtran
\begin_preamble
\usepackage{epsfig} 
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Large-scale synthesis of functional spiking neural circuits 
\end_layout

\begin_layout Author
Terry Stewart and Chris Eliasmith
\end_layout

\begin_layout Abstract
In this paper we review the theoretical and software tools used to Spaun,
 currently the largest functional brain model.
 Specifically, we discuss the Neural Engineering Framework (NEF), a mathematical
 theory that provides methods for systematically generating biologically
 plausible spiking networks to implement nonlinear and linear dynamical
 systems.
 NEF models can be used to perform a wide variety of information processing
 functions.
 We then discuss the Semantic Pointer Architecture (SPA), which is a proposal
 regarding some aspects of the organization, function, and representational
 resources used in the mammalian brain.
 We conclude by discussing Spaun, which is an example model that uses the
 SPA and is implemented using the NEF.
 Spaun performs eight different perceptual, motor, and cognitive tasks using
 2.5 million spiking neurons.
 Spaun does not change between tasks, while capturing a wide variety of
 behavioral and neural data.
 Throughout we discuss the software tool Nengo, which allows for the synthesis
 and simulation of neural models efficiently on the scale of Spaun, and
 provides support for constructing models using the NEF and SPA.
 We propose that the NEF/SPA/Nengo combination provides a useful tool set
 for those wishing to explore large-scale neural computation and its connection
 to function and thus behavior.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
In this paper, we describe the methodology and tools we have developed for
 building large-scale systems from simulated spiking neurons.
 In particular, we describe two theoretical tools (the Neural Engineering
 Framework and the Semantic Pointer Architecture) and one software suite
 (Nengo) that we used to contruct what is currently the world's largest
 brain model capable of performing a variety of tasks.
 This model, which we refer to as the Semantic Pointer Architecture Unified
 Network (or Spaun), consists of 2.5 million simulated spiking neurons whose
 properties and interconnections are consistent with those found in the
 human brain.
 The model receives input in the form of digital images on a virtual retina
 and produces output that controls a simulated arm.
 With this framework, Spaun is able to perform eight different tasks, including
 digit recognition, serial working memory, pattern completion, mental arithmetic
, and question answering.
 Furthermore, it is able to switch between these tasks based on its own
 visual input, meaning that there are no external modifications made to
 the network between tasks.
 This sort of cognitive flexibility is a hallmark of cognitive systems,
 but is difficult to achieve with traditional neural modeling approaches.
 
\end_layout

\begin_layout Standard
The theoretical frameworks and software tools we describe in this paper
 are general-purpose, in that they are suitable for the creation of further
 biologically realistic spiking neuron models capable of cognitive processing.
 We begin in section 2 with the Neural Engineering Framework, a 
\begin_inset Quotes eld
\end_inset

neural compiler
\begin_inset Quotes erd
\end_inset

 capable of taking a vector-based description of a system (and its dynamics)
 and converting it into a spiking neural network.
 In section 3 we describe the Semantic Pointer Architecture, a method for
 taking cognitive algorithms and converting them into a vector-based description
 consistent with mammalian neurobiological constraints.
 Sections 2 and 3 both end with descriptions of our open-source software
 that implements these ideas, Nengo.
 Finally, in section 4 we show how these tools work can be used in concert
 to produce Spaun.
 Material throughout this paper is adapted from 
\begin_inset CommandInset citation
LatexCommand cite
key "Eliasmith2013,Eliasmith2012b,rasmussen2013modeling"

\end_inset

.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:The-Neural-Engineering"

\end_inset

The Neural Engineering Framework (NEF)
\end_layout

\begin_layout Standard
The Neural Engineering Framework (NEF) is a general-purpose system for taking
 algorithms and implementing them using spiking neurons 
\begin_inset CommandInset citation
LatexCommand cite
key "Eliasmith2003m"

\end_inset

.
 This can be thought of as a 
\begin_inset Quotes eld
\end_inset

neural compiler
\begin_inset Quotes erd
\end_inset

 where algorithms written in a high-level language are converted into neurons
 with connections between them.
 This compilation process works for arbitrary neuron types, and can be constrain
ed in biologically realistic ways.
 Importantly, the high-level algorithms must be expressed in terms of vectors
 and functions on those vectors (including ordinary differential equations).
 The resulting neural networks approximate the desired functions, and the
 accuracy of this approximation can be made arbitrarily small by increasing
 the number of neurons.
 This makes the NEF ideal for expressing algorithms typically seen in domains
 such as control theory, and determining their relevance to brain function.
\end_layout

\begin_layout Standard
While the NEF can be used to build arbitrary abstract systems such as controlled
 attractor networks 
\begin_inset CommandInset citation
LatexCommand cite
key "Eliasmith2005p"

\end_inset

, we have primarily used it to show how particular capabilities found in
 real animals are implemented biologically.
 This has included path integration in rodents 
\begin_inset CommandInset citation
LatexCommand citep
key "Conklin2005b"

\end_inset

, working memory 
\begin_inset CommandInset citation
LatexCommand citep
key "Singh2006b"

\end_inset

 and arm movements 
\begin_inset CommandInset citation
LatexCommand cite
key "Dewolf"

\end_inset

 in monkeys, and decision-making in rats 
\begin_inset CommandInset citation
LatexCommand citep
key "Laubach2010,Liu2011"

\end_inset

 and humans 
\begin_inset CommandInset citation
LatexCommand citep
key "Litt2008u"

\end_inset

.
 We have also taken into account biological constraints such a Dale's Principle
 
\begin_inset CommandInset citation
LatexCommand citep
key "Parisien2008c"

\end_inset

 and incorporated biologically realistic learning rules to construct these
 networks 
\begin_inset CommandInset citation
LatexCommand cite
key "MacNeil2011a,bekolay2013"

\end_inset

.
 Several overviews of the NEF are available 
\begin_inset CommandInset citation
LatexCommand cite
key "Eliasmith2003f,Eliasmith2005p,stewart2012b,Eliasmith2012b"

\end_inset

.
 The rest of this section serves an engineering-oriented summary of the
 three main principles within the NEF, plus our software tool Nengo.
 
\end_layout

\begin_layout Subsection
Principle 1 - Representation
\end_layout

\begin_layout Standard
The core of the NEF is the idea that groups of neurons represent vectors,
 and connections between groups of neurons compute functions on those vectors.
 The first NEF principle shows how the activity of a group of neurons can
 be said to represent a vector, and how changes in the activity of those
 neurons corresponds to changes in the represented vector.
\end_layout

\begin_layout Standard
We start with the notion of a 
\begin_inset Quotes eld
\end_inset

preferred direction vector
\begin_inset Quotes erd
\end_inset

.
 In the brain many neurons have a particular stimulus (or response) for
 which they will fire most quickly.
 As the stimulus (or response) changes to become less similar to the preferred
 vector, the neuron will fire less quickly.
 This was originally identified in the motor system 
\begin_inset CommandInset citation
LatexCommand cite
key "Georgopoulos1989q"

\end_inset

 and has since been seen in the head direction system 
\begin_inset CommandInset citation
LatexCommand cite
key "Taube2007"

\end_inset

, visual system 
\begin_inset CommandInset citation
LatexCommand cite
key "Rust2006"

\end_inset

, and auditory system 
\begin_inset CommandInset citation
LatexCommand cite
key "Fischer2009w"

\end_inset

.
 For a more detailed exploration of this idea, see 
\begin_inset CommandInset citation
LatexCommand citep
key "Stewart2011a"

\end_inset

.
\end_layout

\begin_layout Standard
For the NEF, we generalize this idea to all neural populations.
 In particular, we quantify this by stating that the total current going
 into a neuron will be proportional to the dot product of the vector to
 be represented, 
\begin_inset Formula $\mathbf{x}$
\end_inset

, and the preferred direction vector for the neuron, 
\begin_inset Formula $\mathbf{e}_{i}$
\end_inset

 (plus a constant bias term).
 The response of a neuron 
\series bold

\begin_inset Formula $i$
\end_inset


\series default
 for any given input vector 
\series bold

\begin_inset Formula $\mathbf{x}$
\end_inset


\series default
 is thus
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
a_{i}(\mathbf{x})=G_{i}[\alpha_{i}\mathbf{e}_{i}\mathbf{x}+J_{i}^{bias}]\label{eq:enc}
\end{equation}

\end_inset

where 
\begin_inset Formula $a$
\end_inset

 is the spiking output of the neuron, 
\begin_inset Formula $G$
\end_inset

 is the neuron model, 
\begin_inset Formula $\alpha$
\end_inset

 is a randomly chosen gain term, 
\begin_inset Formula $\mathbf{e}$
\end_inset

 is the preferred direction vector, and 
\begin_inset Formula $J^{bias}$
\end_inset

 is a randomly chosen fixed background current.
 We use 
\begin_inset Formula $\mathbf{e}$
\end_inset

 for 
\shape italic
encoder
\shape default
 to indicate that (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:enc"

\end_inset

) captures a transformation between spaces: 
\series bold

\begin_inset Formula $\mathbf{x}$
\end_inset


\series default
 is encoded in the activity space 
\begin_inset Formula $a_{i}$
\end_inset

 of the neurons.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:encoding"

\end_inset

 shows this encoding for the case where 
\begin_inset Formula $\mathbf{x}$
\end_inset

 is two-dimensional and the neural population consists of four neurons.
 Importantly, the NEF applies to a large variety of neuron models (including
 both spiking and non-spiking models) since it makes no commitment to a
 specific function 
\begin_inset Formula $G$
\end_inset

 (whose input is the total current flowing into the neuron).
 The leaky-integrate-and-fire (LIF) model is a common choice, for reasons
 of computational efficiency (and is used in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:encoding"

\end_inset

), but a wide variety of neural models work with the NEF.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 2d Representation.eps
	width 3.5in

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:encoding"

\end_inset

NEF encoding in two dimensions with four neurons.
 a) Both dimensions of the input plotted on the same graph, over 1.2s.
 The input to the two dimensions is 
\begin_inset Formula $x_{1}=\sin(6t)$
\end_inset

 (black) and 
\begin_inset Formula $x_{2}=\cos(6t)$
\end_inset

 (gray).
 b) The spikes generated by the neurons in the group driven by the input
 in a).
 c) The same input shown in the vector space.
 The path of the input is a unit circle, where the arrowhead indicates the
 vector at the end of the run, and the direction of movement.
 Older inputs are in progressively lighter gray.
 The preferred direction vectors of all four neurons is also shown.
 d) The firing rate tuning curves of all four neurons.
 Gains and biases are randomly chosen.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Given an encoding operation, it is natural to define a decoding operation,
 in order to characterizing the information processing characteristics of
 the system (in this case an ensemble of neurons).
 For reasons that will be apparent in a moment, we use a 
\shape italic
linear
\shape default
 decoder:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{\mathbf{x}}=\sum_{i}^{N}a_{i}\mathbf{(x)}\mathbf{d}_{i}\label{eq:dec}
\end{equation}

\end_inset

where 
\begin_inset Formula $N$
\end_inset

 is the number of neurons in the group, 
\begin_inset Formula $\mathbf{d}_{i}$
\end_inset

 are the linear decoders, and 
\begin_inset Formula $\hat{\mathbf{x}}$
\end_inset

 is the estimate of the input driving the neurons.
 
\end_layout

\begin_layout Standard
Any optimization method can be used to find these decoders.
 The simplest is to use standard least-squares optimization:
\begin_inset Formula 
\begin{equation}
E=\frac{1}{2}\int[\mathbf{x}-\sum_{i}a_{i}(\mathbf{x)}\mathbf{d}_{i}]^{2}d\mathbf{x}\label{eq:error}
\end{equation}

\end_inset

where 
\begin_inset Formula $\mathbf{d}_{i}$
\end_inset

 are the decoding vectors over which this error is minimized.
 It has been shown that linear decoders are sufficient to decode ~95% of
 the information available in a spike train generated by a stimulus 
\begin_inset CommandInset citation
LatexCommand cite
key "Rieke1997b"

\end_inset

.
 The NEF decoding process is depicted in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:NEF-decoding"

\end_inset

, where the optimal linear decoders have been found and used for twenty
 neurons.
 Temporal decoding is also linear, and is performed using the post-synaptic
 current-based filters (see section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Principle-3--"

\end_inset

 for further discussion).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 2D Representation Decoding.eps
	width 3.5in

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:NEF-decoding"

\end_inset

NEF decoding in two dimensions with 20 neurons.
 The inputs in the vector space are the same as in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:encoding"

\end_inset

.
 a) The original input and neuron estimate over 1.2s, with both dimensions
 plotted on the same graph over time (black is x1, gray is x2).
 b) The same data shown in the vector space.
 Older states are lighter gray.
 For both a and b, smooth lines represent the ideal 
\series bold
x 
\series default
values, while noisy lines represent the estimate 
\begin_inset Formula $\hat{\mathbf{x}}$
\end_inset

.
 c) The spikes generated by the 20 neurons during the simulation, and used
 to generate the decodings shown in a) and b).
 Encoding vectors are randomly chosen from an uniform distribution around
 the unit circle, and gains and biases are also randomly chosen, as in Spaun.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
While decoders are useful for visualizing the information encoded within
 the activity of a group of neurons, they also provide a direct way to solve
 for connection weights between groups of neurons.
 This is a key advantage of the NEF: rather than using a learning rule to
 optimize over the entire space of all connection weights, we instead solve
 the simpler problem of optimizing over the space of decoders, and then
 use that result to solve for the connection weights.
 Importantly, given the characterization of neural activity with preferred
 direction vectors, there is no difference between using this smaller space
 and using the equivalent full connection matrix.
\end_layout

\begin_layout Standard
For example, if a connection between neural groups is meant to compute the
 identity function 
\begin_inset Formula $\mathbf{y}=\mathbf{x}$
\end_inset

 (where 
\begin_inset Formula $\mathbf{y}$
\end_inset

 is the vector space represented by the second population B and 
\begin_inset Formula $\mathbf{x}$
\end_inset

 is the vector space represented by the first population A), the connections
 between individual neurons are given by 
\begin_inset Formula 
\begin{equation}
\omega_{ij}=\mathbf{d}_{i}\alpha_{j}\mathbf{e}_{j}\label{eq:weights}
\end{equation}

\end_inset

where 
\begin_inset Formula $i$
\end_inset

 indexes the neurons in group A and 
\begin_inset Formula $j$
\end_inset

 indexes the neurons in B.
 Behavior of this network is shown in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Using-NEF-weights"

\end_inset

a.
 
\end_layout

\begin_layout Standard
While the least-squares method for optimization we use here is not biologically
 plausible on its own, we have also shown that biologically realistic learning
 rules will converge on a similar solution 
\begin_inset CommandInset citation
LatexCommand cite
key "MacNeil2011a"

\end_inset

.
 These realistic rules are, however, several orders of magnitude more computatio
nally expensive.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 2D Communication Channel.eps
	width 3.5in

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Using-NEF-weights"

\end_inset

Using NEF derived connection weights to compute functions between neural
 populations representing 2-dimensional vectors.
 a) Computing the identity function between A and B.
 b) Computing the element-wise square between A and B.
 These simulations are 1.2s long.
 Both populations have 20 neurons, with randomly chosen encoders, gains
 and biases.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Principle 2 - Transformation
\end_layout

\begin_layout Standard
Connections between groups of neurons can also compute functions other than
 the identity function.
 We do this by finding decoders 
\series bold

\begin_inset Formula $\mathbf{d}_{i}^{f}$
\end_inset

 
\series default
for the particular function
\series bold
 
\begin_inset Formula $f(\mathbf{x})$
\end_inset


\series default
 by substituting 
\series bold

\begin_inset Formula $f(\mathbf{x})$
\end_inset

 
\series default
for 
\begin_inset Formula $\mathbf{x}$
\end_inset

 in equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:error"

\end_inset

)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
E=\frac{1}{2}\int[f(\mathbf{x})-\sum_{i}a_{i}(\mathbf{x)}\mathbf{d}_{i}^{f}]^{2}d\mathbf{x}.\label{eq:error-fcns}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The connection weights can then be computed using (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:weights"

\end_inset

).
 For linear functions
\series bold
 
\begin_inset Formula $f(\mathbf{x})=\mathbf{L}\mathbf{x}$
\end_inset


\series default
 we can put 
\begin_inset Formula $\mathbf{L}$
\end_inset

 directly into the weight equation itself, rather than solving for a new
 decoder.
 In general, the neural connection weights needed to compute the function
\series bold
 
\begin_inset Formula $\mathbf{y=L}f(\mathbf{x\textrm{)}}$
\end_inset


\series default
 are:
\begin_inset Formula 
\begin{equation}
\omega_{ij}=\alpha_{j}\mathbf{d}_{i}^{f}\mathbf{L}\mathbf{e}_{j}\label{eq:weights-general}
\end{equation}

\end_inset

Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Using-NEF-weights"

\end_inset

b shows the computation of the element-wise square (
\begin_inset Formula $f(\mathbf{x})=[x_{1}^{2},x_{2}^{2}]$
\end_inset

).
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "sub:Principle-3--"

\end_inset

Principle 3 - Dynamics
\end_layout

\begin_layout Standard
While the first two principles are sufficient to build neural approximations
 of any desired function of the vector 
\begin_inset Formula $\mathbf{x}$
\end_inset

, the NEF also provides a method for computing functions of the form 
\series bold

\begin_inset Formula $\mathbf{\frac{\mathrm{d}x}{\mathrm{dt}}=}f(\mathbf{x},\mathbf{u})$
\end_inset


\series default
, where 
\begin_inset Formula $\mathbf{u}$
\end_inset

 is the input from some other population.
 We do this by exploiting the fact that neurons do not simply accept input
 as spikes.
 Rather, when a spike is transmitted from one neuron to another, the actual
 current that flows into the second neuron is a low-pass-filtered version
 of that spike.
 In particular, the post-synaptic current is well-approximated by 
\begin_inset Formula $h(t)=e^{-t/\tau}$
\end_inset

, where 
\begin_inset Formula $\tau$
\end_inset

 is the time constant of the neurotransmitter used.
 This time constant varies throughout the brain, e.g., from 2-5 ms (AMPA;
 
\begin_inset CommandInset citation
LatexCommand cite
key "jonas1993quantal"

\end_inset

) and ~100ms (NMDA; 
\begin_inset CommandInset citation
LatexCommand cite
key "sah1990properties"

\end_inset

).
 The effect of this filter is that instead of a connection computing the
 function 
\begin_inset Formula $\mathbf{y}(t)=f(\mathbf{x}(t))$
\end_inset

 it will compute 
\begin_inset Formula $\mathbf{y}(t)=f(\mathbf{x}(t))*h(t)$
\end_inset

 (or, in the Laplace domain, 
\begin_inset Formula $\mathbf{Y}(s)=\mathbf{F}(s)H(s)$
\end_inset

).
\end_layout

\begin_layout Standard
We can use the intrinsic dynamics of a neural connection to compute other
 dynamics by adding a connection from a neural population back to itself.
 This recurrent connection determines the network dynamics.
 We have shown 
\begin_inset CommandInset citation
LatexCommand cite
key "Eliasmith2003m,Eliasmith2013"

\end_inset

 that neurons can approximate 
\series bold

\begin_inset Formula $\mathbf{\frac{\mathrm{d}x}{\mathrm{dt}}=}f(\mathbf{x},\mathbf{u})$
\end_inset


\series default
 by using the previous two NEF principles to find connection weights that
 compute this function:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\tau f(\mathbf{x},\mathbf{u})+\mathbf{x}\label{eq:dyn-feedback}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Lorenz attractor and oscillator.eps
	width 7in

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Two-dynamical-systems"

\end_inset

Two dynamical systems implemented with the NEF.
 a) A simple linear harmonic oscillator.
 b) A nonlinear dynamical system, specifically a chaotic Lorenz attractor.
 Both are recurrently coupled populations of neurons, and the NEF is used
 to compute the coupling connections to implement the two different dynamical
 systems.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Two-dynamical-systems"

\end_inset

a shows a single neural population with a recurrent feedback connection
 computing the standard linear oscillator 
\series bold

\begin_inset Formula $\mathbf{\frac{\mathrm{d}x}{\mathrm{dt}}=}[-x_{2},-x_{1}]$
\end_inset


\series default
.
 This same method works for nonlinear functions as well; in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Two-dynamical-systems"

\end_inset

b we show the classic Lorenz attractor 
\series bold

\begin_inset Formula $\mathbf{\frac{\mathrm{d}x}{\mathrm{dt}}=}[10(x_{2}-x_{1}),x_{1}(28-x_{3})-x_{2},x_{1}x_{2}-\frac{8}{3}x_{3}]$
\end_inset


\series default
.
 As a result of this process, the NEF allows for the construction of neural
 models that correspond to a very large family of functions, including those
 typically employed by modern control theory and dynamic systems theory.
\end_layout

\begin_layout Subsection
Neural Engineering Objects (Nengo)
\end_layout

\begin_layout Standard
These three principles are sufficient to implement all of our neural models.
 However, to simplify the process of constructing these models, we have
 developed an open-source software package known as Nengo (Neural ENGineering
 Objects) that creates and runs these models.
 Models can be created in Nengo using a drag-and-drop graphical user interface
 or specified using the Python scripting language.
 Full details and documentation can be found online at http://nengo.ca, and
 in other publications 
\begin_inset CommandInset citation
LatexCommand cite
key "Stewart2009l"

\end_inset

.
\end_layout

\begin_layout Standard
For example, to create the model shown in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Using-NEF-weights"

\end_inset

a, we use the following script:
\end_layout

\begin_layout LyX-Code
net = nef.Network('Identity Function')
\end_layout

\begin_layout LyX-Code
net.make('A', neurons=20, dimensions=1) 
\end_layout

\begin_layout LyX-Code
net.make('B', neurons=20, dimensions=1)
\end_layout

\begin_layout LyX-Code
net.connect('A', 'B') 
\end_layout

\begin_layout Standard
For Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Using-NEF-weights"

\end_inset

b we need to compute the element-wise square.
 This is specified in a Python function as follows:
\end_layout

\begin_layout LyX-Code
net = nef.Network('Identity Function')
\end_layout

\begin_layout LyX-Code
net.make('A', neurons=20, dimensions=1) 
\end_layout

\begin_layout LyX-Code
net.make('B', neurons=20, dimensions=1)
\end_layout

\begin_layout LyX-Code
def square(x):
\end_layout

\begin_layout LyX-Code
    return x[0]*x[0], x[1]*x[1]
\end_layout

\begin_layout LyX-Code
net.connect('A', 'B', func=square) 
\end_layout

\begin_layout Standard
Nengo will automatically solve for the connection weights that will best
 approximate the provided function.
\end_layout

\begin_layout Standard
Nengo also provides an interactive interface for displaying the results
 of a simulation while it is running, allowing for real-time interaction
 with a running model.
 This interface allows the generated plots to be exported, and was used
 to produce the previous figures in this paper (i.e.
 all figures in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:The-Neural-Engineering"

\end_inset

).
 Furthermore, Nengo scales up to our largest models: the 2.5 million neuron
 Spaun model is run in Nengo, and can be downloaded at http://models.nengo.ca/spau
n.
\end_layout

\begin_layout Section
The Semantic Pointer Architecture (SPA)
\end_layout

\begin_layout Standard
While the Neural Engineering Framework specifies how to convert vector-based
 algorithms into spiking neural networks, a separate theory is needed to
 describe cognitive function in terms of vector-based algorithms.
 Our approach to this problem is called the Semantic Pointer Architecture
 (SPA).
 While a full description can be found elsewhere 
\begin_inset CommandInset citation
LatexCommand cite
key "Eliasmith2013"

\end_inset

, the core of our approach is to suggest a vector-based cognitive architecture:
 i.e., a set of basic functional components, each of which can be defined
 in terms of vector operations, and an organization of those components
 that can work together to implement cognitive algorithms.
 In addition to these components, we provide a hypothesis as to how structured
 representations (like sentences) can be represented using vectors and what
 basic operations need to be performed on those vectors to achieve memory,
 planning, pattern matching, and other behaviors.
 We refer to our proposed form of neurally plausible representation as 
\begin_inset Quotes eld
\end_inset

semantic pointers.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
A generic schematic of a particular cognitive component can be seen in Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:SPA-subsystem-interface"

\end_inset

.
 This core structure can be used for vision, audition, motor control, working
 memory, pattern completion, and so on.
 The details, of course, vary by function as discussed below.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename SPA-subsystem-interface.eps
	width 3.5in

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:SPA-subsystem-interface"

\end_inset

 A schematic for a subsystem of the Semantic Pointer Architecture.
 A high-dimensional representation acts as an interface to the environment,
 or another subsystem, is compressed through a hierarchical structure, generatin
g semantic pointers.
 Moving up or down in this hierarchy compresses or dereferences the semantic
 pointer representations.
 Throughout the hierarchy, the generated semantic pointers can be extracted
 and transformed by other elements of the system (rounded box).
 All transformations are updateable by error signals, some of which come
 from the action selection component, and some of which may be internally
 generated.
 The action selection component influences routing of information throughout
 the subsystem.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:the-entire-SPA"

\end_inset

 combines these basic components together with a control system.
 The separate components can be thought of as physically distinct areas
 of the brain, each of which can perform one type of operation on its inputs.
 The control system is responsible for ensuring that the right information
 is routed to the right component at the right time.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename the-entire-SPA.eps
	width 3.5in

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:the-entire-SPA"

\end_inset

A schema of Semantic Pointer Architecture models.
 This figure consists of several interacting subsystems of the type depicted
 in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:SPA-subsystem-interface"

\end_inset

.
 Dark black lines are projections carrying semantic pointer representations
 between parts of the system, while thinner lines indicate control and error
 signals.
 An 
\begin_inset Quotes eld
\end_inset

internal subsystem
\begin_inset Quotes erd
\end_inset

 is included to highlight the critical role of systems concerned with functions
 like working memory, encoding a conceptual hierarchy, etc.
 Not all information flow is intended to be captured by this schema.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The 
\begin_inset Quotes eld
\end_inset

semantic pointers
\begin_inset Quotes erd
\end_inset

 used by the SPA are compressed representations, where the compression 
\shape italic
maintains similarity information
\shape default
.
 That is, semantic pointers are vectors that are formed by taking input
 vector(s) and producing a lower-dimensional representation of the original
 high-dimensional representation.
 While this compression is assumed to be 
\begin_inset Quotes eld
\end_inset

lossy
\begin_inset Quotes erd
\end_inset

, it is generally possible to recover the original high-dimensional representati
on (or something very similar).
 Because of this compression/decompression relationship, similar inputs
 will produce similar outputs.
\end_layout

\begin_layout Standard
In short, semantic pointers are compact ways of referencing large amounts
 of data; consequently they function similarly to 
\begin_inset Quotes eld
\end_inset

pointers
\begin_inset Quotes erd
\end_inset

 as understood in computer science.
 Typically, in computer science a 
\begin_inset Quotes eld
\end_inset

pointer
\begin_inset Quotes erd
\end_inset

 is the address of some large amount of data stored in memory.
 Pointers are easy to transmit, manipulate and store, so they can act as
 an efficient proxy for the data they point to.
 Semantic pointers provide the same kind of efficiency benefits in a neural
 setting.
\end_layout

\begin_layout Standard
Unlike pointers in computer science, however, semantic pointers are 
\emph on
semantic
\emph default
.
 That is, they are systematically related to the information that they are
 used to reference.
 This means that semantic pointers carry similarity information that is
 derived from their source, in contrast to an arbitrary index that does
 not contain semantic information, as generally used in modern computers.
 
\end_layout

\begin_layout Standard
In our models, we use a variety of compression operations.
 For vision (and other sensory modalities), we learn this compression from
 the structure of the sensory input.
 This is the same approach taken by vision researchers who use Deep Belief
 Networks 
\begin_inset CommandInset citation
LatexCommand cite
key "Hinton2006,Tang2010"

\end_inset

.
 However, for cognitive operations such as combining concepts together into
 structured representations, we use a particular mathematical function,
 as discussed in the next section.
 Importantly, this function (circular convolution) is straightforward for
 neurons to accurately compute given the NEF described above (section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:The-Neural-Engineering"

\end_inset

).
 
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "sub:Structure"

\end_inset

Structure
\end_layout

\begin_layout Standard
A central concern for modeling cognitive processing using neurons (or vectors)
 is how to effectively represent structured information.
 Structure is vital to explanations of cognitive behavior 
\begin_inset CommandInset citation
LatexCommand citep
key "Anderson2007"

\end_inset

.
 As an example, consider the sentence 
\begin_inset Quotes eld
\end_inset

cats chase mice.
\begin_inset Quotes erd
\end_inset

 If this is to be represented as a vector, then we need to represent it
 in such as way that 
\begin_inset Quotes eld
\end_inset

cats chase mice
\begin_inset Quotes erd
\end_inset

 is different from 
\begin_inset Quotes eld
\end_inset

mice chase cats.
\begin_inset Quotes erd
\end_inset

 In an artificial language, like those typically used in computers, such
 a phrase may be represented with a structured representation like 
\family typewriter
chase(cats, mice)
\family default
.
 The majority of theories that attempt to explain human cognition rely on
 the ability to store and manipulate these representations.
 However, the problem of whether, or how, neurons could possibly perform
 such manipulations has lead to a long-standing debate in cognitive science
 
\begin_inset CommandInset citation
LatexCommand citep
key "Fodor1988p"

\end_inset

.
 
\end_layout

\begin_layout Standard
The approach we take is to define two different compression operators 
\begin_inset CommandInset citation
LatexCommand cite
key "Plate1994,Stewart2011a"

\end_inset

.
 The first of these is simple vector addition.
 This takes in two vectors and produces a single new vector as output.
 If each of the terms to be combined together are themselves vectors, then
 we could write the full sentence as follows, where terms in bold are particular
 vectors for each concept:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbf{cats+chase+mice}
\]

\end_inset


\end_layout

\begin_layout Standard
However, this cannot serve to represent structure, since with this approach
 
\begin_inset Quotes eld
\end_inset

cats chase mice
\begin_inset Quotes erd
\end_inset

 would be exactly equal to 
\begin_inset Quotes eld
\end_inset

mice chase cats.
\begin_inset Quotes erd
\end_inset

 As a result we need a second 
\begin_inset Quotes eld
\end_inset

binding
\begin_inset Quotes erd
\end_inset

 operator: an operator that takes two vectors as input and produces a third
 that is very dissimilar to the original inputs (as opposed to vector addition,
 which produces an output that is highly similar to the inputs).
 Denoting this operation as 
\begin_inset Formula $\mathbf{\circledast}$
\end_inset

, and introducing new vectors for the 
\shape italic
roles
\shape default
 that terms in the sentence take on, we can represent the sentence 
\begin_inset Formula $\mathbf{S}$
\end_inset

 as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbf{S}=\mathbf{agent}\circledast\mathbf{cats}+\mathbf{verb}\circledast\mathbf{chase}+\mathbf{object}\circledast\mathbf{mice}
\]

\end_inset


\end_layout

\begin_layout Standard
Importantly, we also need to reverse (i.e.
 decompress) these operations.
 Given a sentence, we need to be able to identify what the verb is, for
 example.
 For this, we need an inverse operation such that 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbf{S}\circledast\mathbf{verb}'\approx\mathbf{chase}
\]

\end_inset

 where 
\begin_inset Formula $\mathbf{verb}'$
\end_inset

 is an inverse of 
\series bold

\begin_inset Formula $\mathbf{verb}$
\end_inset


\series default
.
\end_layout

\begin_layout Standard
There are a number of different vector operations that can fulfill the role
 of a binding operator, and this family of approaches are known as Vector
 Symbolic Architectures (VSAs) 
\begin_inset CommandInset citation
LatexCommand citep
key "Gayler2003l"

\end_inset

.
 One that is natural to implement in neurons via the NEF is circular convolution
, which was originally explored by Plate in his Holographic Reduced Representati
ons 
\begin_inset CommandInset citation
LatexCommand cite
key "Plate1991g"

\end_inset

.
 To efficiently implement this operation in neurons, we note that a) circular
 convolution is elementwise multiplication in the Fourier transform space,
 and b) the Fourier transform of a vector is a linear operation (multiplication
 by 
\begin_inset Formula $\mathbf{F}$
\end_inset

, the discrete Fourier transform matrix).
 Thus the binding of any two vectors, 
\begin_inset Formula $\mathbf{A}$
\end_inset

 and 
\begin_inset Formula $\mathbf{B}$
\end_inset

, can be computed by 
\begin_inset Formula 
\[
\mathbf{C}=\mathbf{A}\circledast\mathbf{B}=\mathbf{F}^{-1}(\mathbf{FA}\odot\mathbf{FB})
\]

\end_inset

where 
\begin_inset Formula $\mathbf{\odot}$
\end_inset

 is used to indicate element-wise multiplication of the two vectors (i.e.,
 
\begin_inset Formula $\mathbf{x}\odot\mathbf{y}=(x_{1}y_{1},\ldots,x_{n}y_{n}$
\end_inset

)).
 Given the NEF, this is easily computed using a standard, two-layer feedforward
 network (see figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Network-architecture-binding"

\end_inset

).
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Binding-vectors"

\end_inset

 shows the results of binding four sets of vectors using the circular convolutio
n method in spiking neurons.
 The approximate nature of the binding is evident from the 
\begin_inset Quotes eld
\end_inset

clouds
\begin_inset Quotes erd
\end_inset

 of bindings.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename convolution-network.eps
	width 3.5in

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Network-architecture-binding"

\end_inset

The network architecture of a binding network.
 The network is a simple two-layer, feedforward network, as there are connection
 weights into the Bind and C layers.
 In this particular network, A, B, and C have 150 neurons and Bind has 760
 neurons.
 This network is binding two 8D vectors projected into the A and B neurons,
 whose decoded values are shown on the left side.
 These neurons project to the Bind layer, which forms a representation that
 allows the computation of the necessary nonlinearities.
 The spiking activity is shown for 38 randomly selected Bind neurons.
 This activity drives the C layer, which extracts the binding from that
 representation.
 The decoding of the C-layer vector is shown in the top right graph, as
 another 8D vector.
 The results for 200ms of simulation time are shown (from 
\begin_inset CommandInset citation
LatexCommand cite
key "Eliasmith2013"

\end_inset

).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Binding-vectors.eps
	width 3.5in

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Binding-vectors"

\end_inset

Binding vectors with spiking neurons.
 A 10D vector 
\begin_inset Formula $\mathbf{A}$
\end_inset

 is bound with a 10D vector 
\begin_inset Formula $\mathbf{B}$
\end_inset

 to produce a 10D vector 
\begin_inset Formula $\mathbf{C}$
\end_inset

.
 This figure shows four separate instances of binding.
 In each instance, instantaneous decoded samples are drawn every 10ms from
 a neural representation of vectors 
\begin_inset Formula $\mathbf{A}$
\end_inset

, 
\begin_inset Formula $\mathbf{B}$
\end_inset

, and 
\begin_inset Formula $\mathbf{C}$
\end_inset

 over a period of one second.
 This forms a collection of points that are plotted with a unique marker
 (e.g., the light gray circles in the first plot were bound with the light
 gray circles in the second plot resulting in the group of light gray circles
 in the third plot).
 These samples form a 
\begin_inset Quotes eld
\end_inset

cloud
\begin_inset Quotes erd
\end_inset

 because of the neural variability over time due to spiking and other sources
 of noise.
 To visualize the 10D vectors used in this process, vectors are reduced
 to 2D points using principle component (PC) analysis to preserve as much
 of the variability of the higher-dimensional data as possible.
 Similar vectors thus map to points that are close together in these plots.
 Mean values of the groups of points are given by white circles (from 
\begin_inset CommandInset citation
LatexCommand cite
key "Eliasmith2013"

\end_inset

).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
To unbind vectors (i.e., to extract information out of a sentence), the same
 circular convolution operation is used.
 For the inverse, we use an approximation found by permuting the elements
 of the vector (i.e., the involution).
 This is a linear operation (denoted here as 
\begin_inset Formula $\mathbf{S}$
\end_inset

), so a very similar network can be used to compute the following
\begin_inset Formula 
\[
\mathbf{A}\approx\mathbf{C}\circledast\mathbf{B}'=\mathbf{F}^{-1}(\mathbf{FC}\odot\mathbf{FSB})
\]

\end_inset


\end_layout

\begin_layout Standard
[TODO: note that this can be used for pattern matching -- combining advantages
 of symbol manipulations and vectors -- Raven's Matrices]
\end_layout

\begin_layout Subsection
Control
\end_layout

\begin_layout Standard
The above approach to structured representation can be used for many purposes.
 For example, to remember the list 
\begin_inset Quotes eld
\end_inset

seven, six, four
\begin_inset Quotes erd
\end_inset

 we can represent it as the vector 
\begin_inset Formula $\mathbf{M}=\mathbf{seven\circledast P1+six\circledast P2+four\circledast P3}$
\end_inset

.
 To actually store such a representation in a 
\begin_inset Quotes eld
\end_inset

working memory
\begin_inset Quotes erd
\end_inset

 we can build a network whose dynamics are essentially 
\series bold

\begin_inset Formula $\mathbf{\frac{\mathrm{d}x}{\mathrm{dt}}=}\mathbf{u}$
\end_inset


\series default
 (i.e., an integrator).
 However, in order to use such a component effectively, we need to have
 a means of routing the output of one component to the input of another
 component, depending on task demands.
\end_layout

\begin_layout Standard
This control problem can be broken down into two parts: action selection
 (determining which routing is appropriate right now) and action execution
 (implementing the routing).
 For action selection, we have adopted a biologically plausible approach
 available for a vector-based representation: monitor the current state
 of the system (the outputs from all the processing components in cortex)
 and determine the similarity between this state and the 
\begin_inset Quotes eld
\end_inset

preferred
\begin_inset Quotes erd
\end_inset

 states for each available action.
 This similarity is considered to be the 
\begin_inset Quotes eld
\end_inset

utility
\begin_inset Quotes erd
\end_inset

 of the action.
 Whichever state has the highest utility is the action to be selected.
 
\end_layout

\begin_layout Standard
To compute similarity, we use the dot product implemented direction in connectio
n weights, as it is a linear operator and easily computed with the NEF.
 Unfortunately, determining the maximum value over the elements in a vector
 turns out to be difficult to do in neurons.
 If we use the NEF to directly compute this function, the neural approximation
 is highly inaccurate.
 Making it more accurate by increasing the number of neurons leads to a
 solution that, for only 100 actions, requires more neurons than exist in
 the human brain.
 We have thus adopted an approximation of the maximum function that can
 be more efficiently computed using neurons.
\end_layout

\begin_layout Standard
Specifically, such a function seems to be computed in the brain by the basal
 ganglia, a highly interconnected cluster of brain areas found underneath
 the neocortex and near the thalamus.
 This brain area has been consistently implicated in the ability to choose
 between alternative courses of action.
 Damage to the basal ganglia occurs in several diseases of motor control,
 including Parkinson's and Huntington's diseases, and results in significant
 cognitive defects 
\begin_inset CommandInset citation
LatexCommand citep
key "Frank2006"

\end_inset

.
 Neuroscientists 
\begin_inset CommandInset citation
LatexCommand citep
before "e.g., "
key "Redgrave1999"

\end_inset

 and cognitive scientists 
\begin_inset CommandInset citation
LatexCommand citep
before "e.g., "
key "Anderson2004j"

\end_inset

 consider the basal ganglia as being responsible for action selection in
 both motor and cognitive domains 
\begin_inset CommandInset citation
LatexCommand citep
key "Lieberman2006,Lieberman2007"

\end_inset

.
\end_layout

\begin_layout Standard
Modeling the anatomical and physiological structure of the basal ganglia
 has led to algorithms that effectively approximate the maximum function
 
\begin_inset CommandInset citation
LatexCommand citep
key "Gurney2001"

\end_inset

.
 In such models, the input to the basal ganglia is a vector of the utility
 of each action, and the output is a vector indicating which action to pick.
 Each connection between areas of the basal ganglia computes some simple
 function on a vector.
 Thus, these models are in exactly the form needed for use with the NEF.
 As a result, we have extended these models to create a biologically realistic
 spiking implementation that can process high-dimensional representations.
 The result is a close approximation to a maximum operation, but one where
 there are some situations where more than one action is selected, and some
 where no actions are selected (although these are rare occurences and are
 only seen when actions have highly similar utilities).
 For further details, see 
\begin_inset CommandInset citation
LatexCommand citet
key "Stewart2010a"

\end_inset

.
\end_layout

\begin_layout Standard
To execute the actions chosen by this mechanism, we turn to the well-known
 cortex/basal ganglia/thalamus loop through the brain (see figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:cortex_bg_thalamus"

\end_inset

).
 Roughly speaking, the SPA assumes that cortex provides, stores, and manipulates
 representations, the basal ganglia map current brain states to courses
 of action, and the thalamus applies routing signals to cortical and input
 pathways.
 To route information, we form a connection from the output of one processing
 component to the input of another processing component through an intermediate
 group of neurons.
 Each of these connections is optimized to compute the identity function.
 As a result, we can use the output from the basal ganglia to 
\shape italic
inhibit
\shape default
 the firing of neurons in this intermediate population.
 That is, whenever a particular action is 
\shape italic
not
\shape default
 chosen, the relevant interconnections are all strongly inhibited, stopping
 the intermediate neurons from firing.
 If they do not fire, their input to their target populations will be zero,
 so they will not route their input signal to the target.
 This matches closely with the connectivity and inhibition characteristics
 found in the mammalian brain.
 The basal ganglia thus control the exploitation of cortical resources by
 selecting appropriate motor and cognitive actions, based on representations
 available in cortex itself.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename cortex_bg_thalamus.eps
	width 3.5in

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:cortex_bg_thalamus"

\end_inset

The cortex-basal ganglia-thalamus loop.
 Arrows indicate connections between the three areas.
 At a functional level, brain states from the cortex are mapped through
 the 
\begin_inset Formula $\mathbf{M}_{b}$
\end_inset

 matrix to the basal ganglia.
 Each row in such a matrix specifies a known context for which the basal
 ganglia will choose an appropriate action.
 The product of the current cortical state and 
\begin_inset Formula $\mathbf{M}_{b}$
\end_inset

 provides a measure of how similar the current state is to each of the known
 contexts.
 The output of the basal ganglia disinhibits the appropriate areas of thalamus.
 Thalamus, in turn, is mapped through the matrix 
\begin_inset Formula $\mathbf{M}_{c}$
\end_inset

 back to the cortex.
 Each column of this matrix specifies an appropriate cortical state that
 is the consequence of the selected action.
 The relevant anatomical structures are pictured on the right (from 
\begin_inset CommandInset citation
LatexCommand cite
key "Eliasmith2013"

\end_inset

).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Using this architecture, we can implement a controlled cognitive system.
 To understand how this structure operates, we can begin by thinking of
 it as a neural implementation of a 
\shape italic
production system
\shape default
, a standard approach for explaining human cognition (although the SPA is
 more computationally powerful 
\begin_inset CommandInset citation
LatexCommand cite
after " chp. 7"
key "Eliasmith2013"

\end_inset

).
 A production system consists of a large set of IF-THEN rules.
 At any moment in time only one of those rules can be active (determined
 by which rule's IF condition best matches the current situation).
 When a rule 
\begin_inset Quotes eld
\end_inset

fires
\begin_inset Quotes erd
\end_inset

, the THEN part of that rule is taken to indicate the cognitive action that
 should be taken (e.g., moving information from the visual system to working
 memory, modifying certain information, moving information from working
 memory to the speech areas, etc.).
 Psychologists have consistently inferred from behaviour that the human
 brain requires approximately 50 milliseconds to select and execute such
 a cognitive action 
\begin_inset CommandInset citation
LatexCommand cite
key "Anderson2007"

\end_inset

.
 Similar timing is found in our neural implementation 
\begin_inset CommandInset citation
LatexCommand cite
key "Stewart2010a"

\end_inset

, but in that case the timing is due to the neurotransmitter time constants
 measured from the relevant physiological characteristics of neurons.
\end_layout

\begin_layout Standard
In building this neural architecture, we generalize the IF portion of a
 rule to be a utility calculation, and the THEN portion of the rule to be
 a routing control signal.
 As an example, consider the following rule: 
\end_layout

\begin_layout Quotation
\begin_inset Formula $U_{i}=\mathbf{visual}\cdot(\mathbf{A+B+C+D+...)}$
\end_inset


\end_layout

\begin_layout Quotation
\begin_inset Formula $R_{i}:\mathbf{visual\rightarrow memory}$
\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $U_{i}$
\end_inset

 is the utility of the 
\begin_inset Formula $i$
\end_inset

th rule and 
\begin_inset Formula $R_{i}$
\end_inset

 is the routing implemented by that rule.
 This rule says that if the visual system currently contains a letter (i.e.,
 any of the semantic pointers for A, B, C, D, and so on), then remember
 that letter.
 Note that even if the vector stored in the visual field is not exactly
 one of these letters, this rule will still have a high utility and will
 be chosen by the basal ganglia (assuming no other rule has a higher utility).
\end_layout

\begin_layout Standard
We can also add other rules; for instance, rules to traverse the alphabet.
 The easiest way to implement this would be to add 25 rules of the same
 form as the following, which says 
\begin_inset Quotes eld
\end_inset

if the item in memory is G, then replace it with H
\begin_inset Quotes erd
\end_inset

:
\end_layout

\begin_layout Quotation
\begin_inset Formula $U_{i}=\mathbf{memory}\cdot\mathbf{G}$
\end_inset


\end_layout

\begin_layout Quotation
\begin_inset Formula $R_{i}:\mathbf{H\rightarrow memory}$
\end_inset


\end_layout

\begin_layout Standard
The result of simulating this simple system can be seen in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:alphabet_routing"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename alphabet_routing.eps
	width 3.5in

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:alphabet_routing"

\end_inset

The consequences of routing information in a simple sequence of actions.
 The contents of working memory are shown on top, and is indicated by decoding
 spiking activity from the memory population of neurons (via Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:dec"

\end_inset

) and comparing that decoded value to the ideal vector for each letter (via
 a dot product).
 The lower half of the graph shows spiking output from the basal ganglia
 indicating the action to perform.
 The 
\begin_inset Formula $\mathbf{visual}$
\end_inset

 action takes information from visual cortex and routes it to working memory
 (in this case 
\begin_inset Formula $\mathbf{F}$
\end_inset

; from 
\begin_inset CommandInset citation
LatexCommand cite
key "Eliasmith2013"

\end_inset

).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We can also introduce routing that manipulates vectors as they are passed
 from one component to another.
 This is vital for performing manipulations on these structured representations.
 For example, we can perform simple question answering by starting with
\end_layout

\begin_layout Quotation
\begin_inset Formula $\mathbf{statement}+\mathbf{blue}\circledast\mathbf{circle}+\mathbf{red}\circledast\mathbf{square}$
\end_inset


\end_layout

\begin_layout Standard
to indicate that there is a blue circle and a red square.
 If we then present the input
\end_layout

\begin_layout Quotation
\begin_inset Formula $\mathbf{question}+\mathbf{red}$
\end_inset


\end_layout

\begin_layout Standard
we expect the system to respond with the vector for 
\begin_inset Formula $\mathbf{square}$
\end_inset

.
 This is the most basic form of question answering, in that this simple
 example does not consider the harder problem of sequentially presenting
 each word individually and building up these representations.
 For a description of the rules and processing units needed to perform this,
 see [refs: Xuan's paper and my paper from CogSci 2013].
\end_layout

\begin_layout Standard
This basic case, however, can be implemented with two basal ganglia rules.
 The first handles the initial statement:
\end_layout

\begin_layout Quotation
UTILITY:
\begin_inset Formula $\mathbf{visual\cdot statement}$
\end_inset


\end_layout

\begin_layout Quotation
ROUTING: 
\begin_inset Formula $\mathbf{visual\rightarrow memory}$
\end_inset


\end_layout

\begin_layout Standard
and the second handles the question, indicating that the question should
 be combined with the memory to produce an answer
\end_layout

\begin_layout Quotation
UTILITY:
\begin_inset Formula $\mathbf{visual\cdot question}$
\end_inset


\end_layout

\begin_layout Quotation
ROUTING:
\begin_inset Formula $\mathbf{memory\circledast visual'\rightarrow output}$
\end_inset


\end_layout

\begin_layout Standard
These two rules will handle any question answering task of this form.
 If we provide the network with the input described above, the first rule
 will cause
\begin_inset Formula $\mathbf{\mathbf{statement}+\mathbf{blue}\circledast\mathbf{circle}+\mathbf{red}\circledast square}$
\end_inset

 to be stored in memory.
 When the question appears, the second rule will cause that value, bound
 with the inverse of the question, to be output.
 This gives the following result:
\end_layout

\begin_layout Quotation
\begin_inset Formula $(\mathbf{statement}+\mathbf{blue}\circledast\mathbf{circle}+\mathbf{red}\circledast\mathbf{square})\circledast(\mathbf{\mathbf{question}+red})\mathbf{'}$
\end_inset


\end_layout

\begin_layout Quotation
\begin_inset Formula $=(\mathbf{statement}+\mathbf{blue}\circledast\mathbf{circle}+\mathbf{red}\circledast\mathbf{square})\circledast(\mathbf{\mathbf{question'}+red'})$
\end_inset


\end_layout

\begin_layout Quotation
\begin_inset Formula $=\mathbf{red}\circledast\mathbf{square}\circledast\mathbf{red'+}\mathbf{red}\circledast\mathbf{square}\circledast\mathbf{question'+...}$
\end_inset


\end_layout

\begin_layout Quotation
\begin_inset Formula $\approx\mathbf{square}\mathbf{+}\mathbf{red}\circledast\mathbf{square}\circledast\mathbf{question'+...}$
\end_inset


\end_layout

\begin_layout Standard
Since the binding operation produces vectors that are highly dissimilar
 to the inputs, the extra terms will all be dissimilar to
\begin_inset Formula $\mathbf{square}$
\end_inset

.
 Therefore, the resulting output vector will be similar to 
\begin_inset Formula $\mathbf{square}$
\end_inset

, in the sense that it will have a large dot product with 
\begin_inset Formula $\mathbf{square}$
\end_inset

.
 We have shown that this approximation is sufficient to accurately store
 and recall up to eight items, given a standard adult vocabulary size of
 60,000 items [reference].
 To achieve this, we need 512-dimensional vectors.
 Interestingly, we find that the neural models generated by the NEF to implement
 circular convolution on 512-dimensional vectors are exactly the right size
 for the connectivity found in human cortex [reference].
 This gives a potential explanation for observed limits on human cognition,
 such as the limited number of items stored in working memory.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Answering-questions.eps
	width 3.5in

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Answering-questions"

\end_inset

Answering two different questions starting from the same statement.
 Gray areas indicate the period during which the stimuli were presented.
 The similarity between the contents of network's output and the top 7 possible
 answers is shown.
 The correct answer is chosen in both cases after about 50ms (from 
\begin_inset CommandInset citation
LatexCommand citealp
key "Stewart2010b"

\end_inset

).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Finally, we note that the 55 million neurons 
\begin_inset CommandInset citation
LatexCommand citep
key "Beckmann1997"

\end_inset

 found in the input nucleous of the basal ganglia gives an upper bound of
 around one million rules for human cognition.
 Determining what these rules may be is an outstanding research question,
 but the action selection system defined here does continue to function
 as expected when scaled up that far.
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "sub:Learning"

\end_inset

Learning
\end_layout

\begin_layout Standard
The NEF allows very large-scale neural models to be built via optimization,
 rather than through online learning, as is standard for neural network
 research.
 However, any complex cognitive system will also need to be able to adapt
 based on its experiences.
 To handle this, our approach is to use the NEF to generate the initial
 connection weights of a model, but then also introduce a learning rule
 on 
\shape italic
some
\shape default
 of those connections.
 Importantly, this rule is not applied everywhere throughout the model;
 as in the real brain, some connections are more maleable than others.
\end_layout

\begin_layout Standard
We have developed a spike-timing dependent plasticity (STDP) learning rule
 that is a more realistic version of the standard Hebbian 
\begin_inset Quotes eld
\end_inset

neurons that fire together, wire together
\begin_inset Quotes erd
\end_inset

 principle 
\begin_inset CommandInset citation
LatexCommand citeyearpar
key "Hebb1949f"

\end_inset

.
 It is a local rule, meaning that all the information needed to implement
 it is available locally at a particular synapse, and it matches observed
 adaptive timing effects in individual neurons [ref: new learning rule].
 This learning rule combines the well-known homeostatic BCM rule 
\begin_inset CommandInset citation
LatexCommand citep
key "Bienenstock1982y"

\end_inset

 with an error-driven spiking rule 
\begin_inset CommandInset citation
LatexCommand citep
key "MacNeil2011a"

\end_inset

, making it a homeostatic, prescribed error sensitivity (hPES) rule.
\end_layout

\begin_layout Standard
The hPES rule simultaneously addresses limitations of both standard Hebbian
 learning rules and STDP.
 In particular, unlike most standard rules hPES is able to account for precise
 spike time data, and unlike most STDP (and standard Hebbian) rules it is
 able to relate synaptic plasticity directly to the vector space represented
 by an ensemble of neurons.
 That is, the rule can tune connection weights that compute nonlinear functions
 of whatever high-dimensional vector is represented by the spiking patterns
 in the neurons.
 Unlike past proposals, hPES is able to do this because it relies on an
 understanding of the NEF decomposition of connection weights.
 This decomposition makes it evident how to take advantage of high-dimensional
 error signals, which past rules have either been unable to incorporate
 or attempted to side-step 
\begin_inset CommandInset citation
LatexCommand citep
key "Gershman2010"

\end_inset

.
 Thus, hPES can be usefully employed in a biologically plausible network
 that can be characterized as representing and transforming a vector in
 spiking neurons.
 This is precisely the kinds of networks we find in the SPA.
 
\end_layout

\begin_layout Standard
We have shown that this rule can learn the connections needed to perform
 arbitrary functions.
 That is, instead of using the NEF equations to solve for connection weights,
 it would also be possible to learn those connections using our hPES rule.
 For example, Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:stdp-network-structure"

\end_inset

 shows this rule learning a two-dimensional identity function.
 This is, of course, much slower than simply using the NEF to solve for
 the resulting connection weights, but it establishes the capability of
 this rule.
 Importantly, learning a function requires an error signal, and the system
 will learn whatever function is consistent with that error signal.
 In the real brain, this error signal can have many forms, but most famously
 one sees modulatory learning associated with the neurotransmitter dopamine
 in both the cortex and basal ganglia.
 This is a reinforcement or reward signal, known to indicate when actions
 go well (or go poorly).
 Dopamine acts to help modify the connections from the rest of the brain
 to the basal ganglia, and this plays a central role in how living creatures
 adjust their actions to deal with a changing environment 
\begin_inset CommandInset citation
LatexCommand citep
key "Hollerman1998"

\end_inset


\begin_inset CommandInset citation
LatexCommand citep
key "Maia2009"

\end_inset

.
 
\end_layout

\begin_layout Standard
We incorporate this learning into the Semantic Pointer Architecture by providing
 a reward signal and using it to adjust the utililty of the rules in the
 basal ganglia.
 This gives the system the capability of reinforcement learning (RL) [ref:
 Sutton and Barto].
 So far, we have only shown basic RL capabilities such as learning which
 of three options gives the best chance of a reward (a three-armed-bandit
 task [ref]).
\end_layout

\begin_layout Standard
[TODO: add a picture of the bandit task learning?]
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename stdp-network-structure.eps
	width 3.5in

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:stdp-network-structure"

\end_inset

A network learning a 2D communication channel.
 a) The network consists of an initially random set of connections between
 the input and output populations that uses hPES to learn a desired function
 (dashed line).
 The 
\begin_inset Quotes eld
\end_inset

err
\begin_inset Quotes erd
\end_inset

 population calculates the difference, or error, between the input and output
 populations, and projects this signal to the output population.
 b) A sample run of the network as it learns a simple 2-dimensional communicatio
n channel between the input and the output.
 Over time, the decoded value of the output population (solid lines) begins
 to follow the decoded value of the input population (dashed lines), meaning
 that the network has learned to represent its input values correctly (from
 
\begin_inset CommandInset citation
LatexCommand cite
key "Eliasmith2013"

\end_inset

).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
SPA in Nengo
\end_layout

\begin_layout Standard
All of the SPA components discussed above can be built in Nengo using the
 scripting system.
 However, to simplify the creation of these cognitive models, we have developed
 an additional library that assists in the creation of systems based on
 this cortex, basal ganglia, and thalamus loop.
 
\end_layout

\begin_layout Standard
Individual processing modules are specified using the standard NEF libraries.
 To connect them, however, we use a custom syntax for defining the rules
 that should be used to route information between areas.
 For example, the following code creates the question answering model from
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Answering-questions"

\end_inset

.
\end_layout

\begin_layout LyX-Code
class Rules:
\end_layout

\begin_layout LyX-Code
    def store(visual='STATEMENT'):
\end_layout

\begin_layout LyX-Code
        set(memory=visual)
\end_layout

\begin_layout LyX-Code
    def recall(visual='QUESTION'):
\end_layout

\begin_layout LyX-Code
        set(output=memory*~visual)
\end_layout

\begin_layout LyX-Code

\end_layout

\begin_layout LyX-Code
class QuestionAnsweringModel(spa.SPA):
\end_layout

\begin_layout LyX-Code
    dimensions = 512                   
\end_layout

\begin_layout LyX-Code
    visual = spa.Buffer(feedback=0)     
\end_layout

\begin_layout LyX-Code
    output = spa.Buffer(feedback=0)
\end_layout

\begin_layout LyX-Code
    memory = spa.Buffer(feedback=1)             
\end_layout

\begin_layout LyX-Code
    
\end_layout

\begin_layout LyX-Code
    bg = spa.BasalGanglia(Rules)
\end_layout

\begin_layout LyX-Code
    thalamus = spa.Thalamus(bg)
\end_layout

\begin_layout LyX-Code

\end_layout

\begin_layout LyX-Code
net = nef.Network('Question Answering')
\end_layout

\begin_layout LyX-Code
model = QuestionAnsweringModel(net)
\end_layout

\begin_layout Standard
In this model, the initial Rules specify the two routing rules discussed
 above.
 The second part of the code describes the cortical structure of the system.
 Here, there are three 
\begin_inset Quotes eld
\end_inset

Buffers
\begin_inset Quotes erd
\end_inset

, which are simple neural ensembles that can store one vector.
 In this case, the number of dimensions for that vector is set to 512.
 The feedback argument specifies whether or not these neural ensemble use
 the dynamics principle to store their own state.
 When set to 0 there is no recurrence.
 As noted in section 2.3, this means that if input is given to the visual
 system it will decay very quickly, due to the short time constant of the
 filter 
\begin_inset Formula $h(t)=e^{-t/\tau}$
\end_inset

introduced by the post-synaptic current.
 However, with feedback equal to 1, the dynamics principle is used to construct
 an integrator (
\series bold

\begin_inset Formula $\mathbf{\frac{\mathrm{d}x}{\mathrm{dt}}=}\mathbf{u}$
\end_inset


\series default
).
 This, ideally, would result in a system with an effectively infinite time
 constant.
 However, the neural approximation of the integrator will never be perfect
 (with a finite number of neurons).
 This means that memories will decay over time, as expected.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Spaun"

\end_inset

Semantic Pointer Architecture Unified Network (Spaun)
\end_layout

\begin_layout Standard
The previous sections described our general approach to building large-scale
 models.
 We identify particular computational functions that need to be computed
 by a cognitive system, we build neural modules that implement those functions,
 and we route information between them using our cognitive control model
 of the basal ganglia.
 This approach has allowed us to build the largest existing brain model
 that is capable of performing cognitive tasks.
 We did this by taking existing models we have built using the NEF and combining
 them into one unified network.
 The resulting 2.5 million neuron model is thus called the Semantic Pointer
 Architecture Unified Network, or 
\begin_inset Quotes eld
\end_inset

Spaun
\begin_inset Quotes erd
\end_inset

 [ref.
\end_layout

\begin_layout Standard
The individual modules of Spaun have all been previously described as separate
 components.
 There is a neural component for vision [ref: one of Charlie Tang's papers],
 recognition [ref: cleanup memory paper].
 serial working memory [ref: Xuan], pattern matching [ref: Dan], reward
 processing [ref: Trevor], and motor control [ref:Travis].
 All together this model includes 20 components, each of which is mapped
 onto a particular anatomical area, and is consistent with the physiological
 properties of the neurons found in those areas.
 Of course, the human brain has many more areas (approximately 1000
\begin_inset CommandInset citation
LatexCommand cite
key "Hagmann2008"

\end_inset

), so there is much work still to be done.
\end_layout

\begin_layout Standard
The key feature of Spaun is that it can use the different neural components
 flexibly, thanks to the control structure.
 Spaun can perform eight different tasks, and each of these tasks use most
 of the cognitive modules, but in their own way.
 For example, in the list memory task, it is shown a list and then it repeats
 it back, while in the question answering task it is first shown a list,
 and then it waits for a question about this list.
 Importantly, we do not make any changes to Spaun to change the task.
 Rather, we 
\shape italic
tell
\shape default
 Spaun to change tasks.
 Since the only input to Spaun is its 28x28 visual field, we tell Spaun
 what task to do by presenting the letter 
\begin_inset Quotes eld
\end_inset

A
\begin_inset Quotes erd
\end_inset

 followed by a number that indicates what task to perform.
 Spaun responds to this command thanks to two control rules:
\end_layout

\begin_layout Quotation
UTILITY:
\begin_inset Formula $\mathbf{visual\cdot A}$
\end_inset


\end_layout

\begin_layout Quotation
ROUTING: 
\begin_inset Formula $\mathbf{NONE\rightarrow task}$
\end_inset


\end_layout

\begin_layout Quotation
UTILITY:
\begin_inset Formula $(\mathbf{visual\cdot(ZERO+ONE+TWO+...)+task\cdot NONE})/2$
\end_inset


\end_layout

\begin_layout Quotation
ROUTING: 
\begin_inset Formula $\mathbf{visual\rightarrow task}$
\end_inset


\end_layout

\begin_layout Standard
The first rule clears (by setting it to the random vector
\series bold

\begin_inset Formula $\mathbf{NONE}$
\end_inset


\series default
) the part of working memory that keeps track of whatever task Spaun is
 currently doing when Spaun sees the letter A.
 The second rule will only have a high utility if the task is 
\series bold

\begin_inset Formula $\mathbf{NONE}$
\end_inset


\series default
 and a number has been seen.
 When this occurs, that number is sent to the task memory.
 To implement the various tasks, more rules are added, each of which has
 as part of their utility calculation a dependency on the task memory.
 This makes the rules task-specific, but the actual neural modules that
 the rules make use of are general across tasks.
 Spaun is able to perceive visual input through a 28x28 pixel retina, remember
 that information, act on it as appropriate, and generate motor output that
 moves a physically-modelled arm to write numbers.
 The eight tasks it can perform (digit recognition, digit style copying,
 list memory, question answering, addition by counting, reinforcement learning,
 pattern completion, and the Raven's Progressive matrix intelligence test)
 are all implemented with only nineteen basal ganglia rules.
 More information on these tasks can be found in [ref: Spaun] and via the
 online videos of Spaun's performance at
\begin_inset Flex Flex:URL
status collapsed

\begin_layout Plain Layout

http://nengo.ca/build-a-brain/spaunvideos
\end_layout

\end_inset

.
 The overall architecture can be seen in Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:spaunarchitecture"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
 
\begin_inset Graphics
	filename spaunarchitecture.eps
	width 3.5in

\end_inset

 
\begin_inset Caption

\begin_layout Plain Layout
Functional and anatomical architecture of Spaun.
 (a) Functional architecture of Spaun.
 The working memory, visual input, and motor output components represent
 hierarchies that compress/decompress neural representations between different
 representational spaces.
 The action selection component chooses which action to execute given the
 current state of the rest of the system.
 The five internal subsystems, from left to right, are used to 1) map visual
 inputs to conceptual representations, 2) induce relationships between represent
ations, 3) associate input with reward, 4) map conceptual representations
 to motor actions, and 5) map motor actions to specific patterns of movement.
 (b) Corresponding neuroanatomical architecture, with matching colors and
 line styles indicating corresponding components.
 Abbreviations: V1/V2/V4 (primary/secondary/extrastriate visual cortex),
 AIT/IT (anterior/inferotemporal cortex), DLPFC/VLPFC/OFC (dorso-lateral/ventro-
lateral/orbito- frontal cortex), PPC (posterior parietal cortex), M1 (primary
 motor cortex), SMA (supplementary motor area), PM (premotor cortex), v/Str
 (ventral/striatum), STN (subthalamic nucleus), GPe/i (globus pallidus externus/
internus), SNc/r (substantia nigra pars compacta/reticulata), VTA (ventral
 tegmental area).
 Reproduced with permission from Eliasmith et al.
\begin_inset CommandInset citation
LatexCommand cite
key "Eliasmith2012a"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:spaunarchitecture"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
As an example of Spaun performing a task, Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:spikeplot"

\end_inset

 shows the list memory task.
 The input starts with an 
\begin_inset Quotes eld
\end_inset

A
\begin_inset Quotes erd
\end_inset

 followed by a 
\begin_inset Quotes eld
\end_inset

3
\begin_inset Quotes erd
\end_inset

, telling Spaun which task to perform.
 Next is a series of numbers that it should attempt to remember.
 When it sees a question mark, it should respond with the list.
 Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:spikeplot"

\end_inset

 shows spiking activity of a few different parts of the model.
 Of particular interest is DLPFC (dorsolateral prefrontal cortex), which
 is where it is storing the list.
 This list is stored as a vector, using the Semantic Pointer approch described
 above.
 The DLPFC graph shows the similarity (as measured by the dot product) between
 the vector stored there (decoded from the spikes using the NEF) and the
 ideal vector for numbers in different positions.
 This activity decays over time and becomes less accurate the more items
 there are in the list.
 In this particular case, this inaccuracy is enough to cause Spaun to make
 a mistake: it forgets the fourth item in the list (the eight).
 This is exactly the sort of mistake typically made by people, and is part
 of how we validate our models.
 After all, if we are accurately modelling human cognition, Spaun should
 make the same sorts of mistakes as people do.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename spikeplot_modified.eps
	lyxscale 25
	width 3.5in

\end_inset

 
\begin_inset Caption

\begin_layout Plain Layout
Recordings from Spaun performing the serial recall task.
 The top row (a and b) shows Spaun's experimental environment.
 The internal processing of the model is shown in the thought bubbles as
 the spiking activity of a component overlaid with the information represented
 by those spikes.
 The brain surface itself shows spatially arranged firing rates.
 a) Spaun observing and processing inputs.
 b) Spaun moving its arm to draw a response.
 c) The neural spiking activity of various components of the model (see
 Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:spaunarchitecture"

\end_inset

 for details).
 In this particular trial Spaun makes a mistake, forgetting the digit in
 the middle of the list (just as human subjects often do).
 The spiking activity in DLPFC and the corresponding decoded information
 reveal the cause of this mistake, showing the representation of the 8 position
 decaying.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:spikeplot"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
One of the key benefits of this type of model is that it provides a comprehensiv
e description of processes ranging from high-level behavior down to the
 level of single neurons.
 For example, at the highest level of the serial recall task the model specifies
 a given input-output mapping (given a list of digits, output the list in
 the same order).
 However, we are not limited to examining the model at that abstract level
 by, for example, noting its match to human error rates.
 We can look within the model to see how the functional components work
 together and process information in order to carry out the necessary computatio
ns (Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:spikeplot"

\end_inset

a/b).
 We can further map those functional components onto neuroanatomical ones
 (Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:spaunarchitecture"

\end_inset

b).
 And we can examine the details of those components, as they are implemented
 by specific neural circuits, allowing us to characterize the flow and processin
g of information at the level of neural ensembles.
 Furthermore, within any of those ensembles, we can record and analyze,
 for example, the dendritic inputs, the membrane voltage, or the spiking
 behavior of single neurons (Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:spikeplot"

\end_inset

c).
 This allows us to demonstrate the model's match to electrophysiological
 data, such as single neuron spike frequency spectrum shifts in monkeys
 performing a similar working memory task.
\end_layout

\begin_layout Standard
This fully mechanistic explanation allows the model to address interesting
 questions about the connections within and between these levels of analysis.
 For example, how do drug-like perturbations to postsynaptic current dynamics
 affect the model's ability to remember lists of digits? How does information
 encoding impairment (e.g., damage to the anterior inferotemporal cortex)
 affect the model's ability to reproduce an observed digit? We are still
 at the early stages of exploring such questions, and there are many more
 to be asked.
 We have made Spaun publicly available, so that other researchers may ask
 their own questions and test the model as they see fit (
\begin_inset Flex Flex:URL
status collapsed

\begin_layout Plain Layout

http://models.nengo.ca/spaun
\end_layout

\end_inset

).
\end_layout

\begin_layout Standard
Large-scale mechanistic neural models present many exciting possibilities
 for understanding the brain.
 A model like Spaun allows us to take an interventionist approach, manipulating
 or interfering with different aspects of the model and examining the result.
 For example, we are currently exploring the effects of ageing, examining
 how neurophysiological changes associated with age (such as neuron loss
 or connectivity decreases) affect behavioral performance when we recreate
 them in the model.
 The detailed neuroanatomical mapping (Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:spaunarchitecture"

\end_inset

) also allows us to explore the functional impact of lesioning specific
 brain regions, allowing us to recreate the effects of strokes or other
 brain damage.
 These explorations are not limited to negative effects; for example, we
 can increase the working memory capacity of the model and examine how that
 affects performance across the eight tasks.
 These are not new questions, but by performing these investigations using
 a mechanistic model it is easier to draw causal---rather than correlational---c
onclusions, since we are directly manipulating the variable of interest
 (which is often not possible in a real brain).
 In addition, such models allow for the investigation of systemic interactions
 across many levels of intervention (e.g., direct brain stimulation, drug-based
 therapies, or behavioral interventions).
\end_layout

\begin_layout Standard
Given the limited behavioral repertoire of current models, we expect much
 future work to be directed at expanding models' functional capabilities.
 For example, we are interested in improving their behavioral flexibility---givi
ng models the ability to dynamically operate in new ways, rather than having
 tasks pre-specified.
 Our research in this area is evolving in two related directions.
 The first is giving models the ability to follow task instructions; that
 is, the model is given a description of a task (
\begin_inset Quotes eld
\end_inset

add together the next two numbers you see then subtract the third
\begin_inset Quotes erd
\end_inset

), and then carries that task out.
 The second direction is to allow models to learn new tasks based on reward.
 In this case the model is not given instructions on how to complete the
 task, only feedback on whether it did the task right or wrong.
 It uses that information to gradually learn to perform the new task successfull
y.
 These capabilities will allow models to develop larger and more fluid behaviora
l repertoires, helping to capture these sophisticated aspects of human behavior.
\end_layout

\begin_layout Standard
As mentioned in Section 3, one of the limitations on existing models is
 simulation speed.
 This restricts the development of larger, more complex models, and also
 prevents Spaun from interacting in real time with its environment.
 These limitations have driven our interest in neuromorphic hardware---custom
 computing hardware designed to simulate million or billions of neurons
 in real time.
 We are collaborating with other research groups
\begin_inset CommandInset citation
LatexCommand cite
key "Merolla2007,Khan2008"

\end_inset

 to combine ideas from theoretical neuroscience with the power of neuromorphic
 hardware.
\end_layout

\begin_layout Standard
In conclusion, the recent Spaun model, while making significant advances
 in connecting our understanding of basic biological components to sophisticated
 behavior, is just a first step.
 It provides tantalizing hints of the utility of large-scale mechanistic
 models for neuroscience, psychology, and artificial intelligence.
 In the coming years we expect to see the potential of these models become
 more widely exploited, enhancing and expanding our basic understanding
 of the brain and helping in the diagnosis and treatment of a wide variety
 of brain disorders.
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "/Users/celiasmi/Documents/library"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
